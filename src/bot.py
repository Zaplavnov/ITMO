import logging

from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes
from telegram.request import HTTPXRequest

from .config import TELEGRAM_BOT_TOKEN, LOG_LEVEL, USE_LLM, OLLAMA_MODEL, HTTP_PROXY
from .retriever import Retriever
from .domain import is_relevant_question, is_recommendation_intent, extract_background_tags, detect_program_from_text
from .recommender import recommend_electives
from .llm import generate_rag_answer

logger = logging.getLogger(__name__)


async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "ÐŸÑ€Ð¸Ð²ÐµÑ‚! Ð¯ Ð¿Ð¾Ð¼Ð¾Ð³Ñƒ Ñ€Ð°Ð·Ð¾Ð±Ñ€Ð°Ñ‚ÑŒÑÑ Ñ Ð¼Ð°Ð³Ð¸ÑÑ‚Ñ€Ð°Ñ‚ÑƒÑ€Ð°Ð¼Ð¸ Ð˜Ð¢ÐœÐž (AI Ð¸ AI Product). Ð—Ð°Ð´Ð°Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸Ð»Ð¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ /recommend."
    )


async def cmd_help(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    await update.message.reply_text(
        "Ð—Ð°Ð´Ð°Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°Ð¼: ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½, Ð´Ð¸ÑÑ†Ð¸Ð¿Ð»Ð¸Ð½Ñ‹, Ñ‚Ñ€ÐµÐºÐ¸. Ð”Ð»Ñ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¹ Ð¿Ð¾ Ð²Ñ‹Ð±Ð¾Ñ€Ð½Ñ‹Ð¼ Ð¿Ñ€ÐµÐ´Ð¼ÐµÑ‚Ð°Ð¼ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ /recommend Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸ ÑÐ²Ð¾Ð¹ Ð±ÑÐºÐ³Ñ€Ð°ÑƒÐ½Ð´."
    )


async def cmd_recommend(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    text = (update.message.text or "").replace("/recommend", "").strip()
    prog = detect_program_from_text(text) or "ai"
    tags = extract_background_tags(text)
    recs = recommend_electives(tags, prog)
    if not recs:
        await update.message.reply_text("ÐŸÐ¾ÐºÐ° Ð½Ðµ Ð½Ð°ÑˆÑ‘Ð» Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ Ð²Ñ‹Ð±Ð¾Ñ€Ð½Ñ‹Ñ… Ð´Ð¸ÑÑ†Ð¸Ð¿Ð»Ð¸Ð½.")
        return
    reply = "\n\n".join([f"â€¢ {r}" for r in recs])
    await update.message.reply_text(reply)


def _format_snippets_fallback(results) -> str:
    """Format search results into readable snippets"""
    lines = []
    for r in results:
        # Clean and truncate text
        text = r.text.strip()
        if len(text) > 300:
            text = text[:300] + "..."
        
        # Add source info
        source = f"ðŸ“š {r.title.replace('_', ' ').title()}"
        lines.append(f"{source}\n{text}\n")
    
    return "\n".join(lines)


async def handle_question(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    query = (update.message.text or "").strip()
    if not query:
        await update.message.reply_text("ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.")
        return
    
    logger.info(f"Processing question: {query}")
    retriever: Retriever = context.application.bot_data.get("retriever")

    if not is_relevant_question(query, retriever):
        logger.info("Question not relevant to ITMO programs")
        await update.message.reply_text("Ð¯ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÑŽ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¿Ð¾ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸ÑŽ Ð½Ð° Ð¼Ð°Ð³Ð¸ÑÑ‚Ñ€Ð°Ñ‚ÑƒÑ€Ð°Ñ… AI Ð¸ AI Product Ð² Ð˜Ð¢ÐœÐž.")
        return

    if is_recommendation_intent(query):
        logger.info("Recommendation intent detected")
        await update.message.reply_text("ÐŸÐ¾Ñ…Ð¾Ð¶Ðµ, Ð½ÑƒÐ¶Ð½Ñ‹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸ Ð¿Ð¾ Ð²Ñ‹Ð±Ð¾Ñ€Ð½Ñ‹Ð¼. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ /recommend Ð¸ Ð¾Ð¿Ð¸ÑˆÐ¸ ÑÐ²Ð¾Ð¹ Ð±ÑÐºÐ³Ñ€Ð°ÑƒÐ½Ð´ Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ.")
        return

    try:
        logger.info("Searching for relevant information")
        results = retriever.search(query, top_k=4)
        if not results:
            logger.warning("No search results found")
            await update.message.reply_text("ÐÐµ Ð½Ð°ÑˆÑ‘Ð» Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² ÑƒÑ‡ÐµÐ±Ð½Ñ‹Ñ… Ð¿Ð»Ð°Ð½Ð°Ñ….")
            return

        logger.info(f"Found {len(results)} search results")
        
        # Try LLM if enabled
        if USE_LLM:
            logger.info("LLM is enabled, attempting generation")
            
            # Notify user that model is processing
            processing_msg = await update.message.reply_text("ðŸ¤– Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ...")
            
            try:
                chunks = [r.text for r in results]
                logger.info(f"Preparing {len(chunks)} chunks for LLM")
                
                answer = generate_rag_answer(query, chunks)
                logger.info(f"LLM generated answer: {len(answer)} chars")
                
                # Append sources
                urls = list(set(r.url for r in results))
                if urls:
                    answer += "\n\nðŸ“– Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:\n" + "\n".join([f"â€¢ {u}" for u in urls])
                
                # Delete processing message and send answer
                await processing_msg.delete()
                await update.message.reply_text(answer)
                logger.info("Successfully sent LLM answer")
                return
                
            except Exception as e:
                logger.error(f"LLM generation failed: {e}", exc_info=True)
                logger.warning("Falling back to snippets due to LLM failure")
                
                # Update processing message to show fallback (ignore network errors)
                try:
                    await processing_msg.edit_text("ðŸ¤– Ð˜Ð˜-Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð°. ÐŸÐ¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ:")
                except Exception as edit_err:
                    logger.warning(f"Failed to edit processing message: {edit_err}")

        # Fallback: show formatted snippets
        logger.info("Using fallback snippets")
        formatted = _format_snippets_fallback(results)
        await update.message.reply_text(formatted)
        logger.info("Successfully sent fallback snippets")
        
    except Exception as e:
        logger.exception(f"Failed to process question: {e}")
        await update.message.reply_text("ÐŸÑ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°.")


def build_app(token: str) -> Application:
    logging.basicConfig(
        level=getattr(logging, LOG_LEVEL, logging.INFO),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    # Configure Telegram HTTP client with timeouts and optional proxy
    request = HTTPXRequest(
        proxy_url=HTTP_PROXY or None,
        connect_timeout=15.0,
        read_timeout=30.0,
        write_timeout=30.0,
        pool_timeout=5.0,
    )
    app = Application.builder().token(token).request(request).build()

    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("help", cmd_help))
    app.add_handler(CommandHandler("recommend", cmd_recommend))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_question))

    return app


def main() -> None:
    if not TELEGRAM_BOT_TOKEN:
        raise RuntimeError("TELEGRAM_BOT_TOKEN Ð½Ðµ Ð·Ð°Ð´Ð°Ð½. Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ ÐµÐ³Ð¾ Ð² .env")

    logger.info("Starting ITMO bot")
    logger.info(f"USE_LLM: {USE_LLM}")
    if USE_LLM:
        logger.info(f"Using Ollama model: {OLLAMA_MODEL}")
    
    retr = Retriever()
    app = build_app(TELEGRAM_BOT_TOKEN)
    app.bot_data["retriever"] = retr

    app.run_polling()


if __name__ == "__main__":
    main()
